---
layout:     post
title:      "机器学习II 模型评估与选择"
subtitle:   " \"Selection and Evaluation\""
date:       2020-05-10 13:23:45
author:     "Durant"
latex: true
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
   - 机器学习
---

## 1. 经验误差与过拟合

学习在训练集上的误差称为“训练误差”，而在新样本上误差称为“泛化误差”。

实际上，我们通常得不到泛化性能很好的模型，原有有两种：

* **过拟合**（overfitting）: 学习能力过于优秀，学到一些非一般的特性。
* **欠拟合**（underfitting）：对训练样本的一般性质尚未学好。

机器学习面临的问题一般是$NP$难问题([什么是NP](https://blog.csdn.net/qq_21768483/article/details/80430590))，因此只要相信$NP\ne P$那么过拟合将无法避免。

下面简单介绍从训练集得到测试集的方法：

1. **留出法**，按照比例从原始数据集分出训练和测试集；
2. **交叉验证**（cross validation） ：先将数据集$D$划分为$k$个大小相似的互斥子集，即$D=D_1\cup D_2 \cup ···\cup D_j , D_i \cup D_j\ne \oslash$,每个子集$D_i$都尽可能保持数据分布的一致性，即从$D$中通过分层采样得到，然后每次用$k-1$给子集的并集作为训练集，余下的那个子集作为测试集；这样就可以得到$k$组训练/测试集。
3. **自助法**（bootstrapping）减少训练集规模不同造成的误差。给定包含$m$个样本的数据集$D$，我们对它采样，产生数据集$D'$：每次随机从$D$中挑选一个样本，将其拷贝放入$D'$，然后再将该样本放回初始数据集$D$，这个过程重复$m$次后，我们得到大小为$m$的数据集$D'$，通过自助采样，约有$36.8\%$的样本未出现在采样数据集中，于是我能将$D'$用作训练集，而$D/D'$作为测试集，这适合数据小，难以划分的场景。

## 2. 调参与最终模型

我们常把学得模型在实际使用中遇到的数据称为**测试数据**。

模型评估与选择中的数据集称为**验证集**。

- **性能度量**

回归任务，最常使用的是均方误差（Mean Squared Error, MSE）.

 																$ E(f;D)=\frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^2$

- **错误率与精度**

错误率是分类错误的样本数占样本总数的比例，精度$acc$则是正确的样本数占样本总数的比例。

> 术语: FP:真正例 FN:假反例 FP:假正例 TN:真反例

查准率$P=\frac{TP}{TP+FP}$，查全率：$R =\frac{TP}{TP+FN}$.

一般我们采用P-R图像来评价机器学习模型的优劣。若一个曲线包住另一个曲线，则证明前者性能好于后者。“平衡点”（BEP）就是查准率等于查全率的点。

更常用的是$F1$度量（调和平均）：$F1=\frac{2×P×R}{P+R}$，其原型是加权调和平均$F_\beta=\frac{(1+\beta^2)×P×R}{\beta^2×P+R}$,$\beta$度量了查全率对查准率的相对重要性。为了更好描述不同错误造成影响的大小，我们引入代价敏感 ，$cost_{01}$表示$TN$的代价,$cost_{10}$表示$FP$的代价。正例代价。
$$
P(+)=\frac{p×cost_{01}}{p×cost_{01}+(1-p)×cost_{10}}

$$


泛化错误率为$\epsilon$,而测试错误率$\hat{\epsilon}$意味着在$m$给测试样本中恰有$\hat{\epsilon}×m$个错误数据。

- **比较检验**

  - 假设检验

  我们在实际任务中不知道学习器的泛化错误率，但是可以根据测试错误率推出泛化错误率的分布

  